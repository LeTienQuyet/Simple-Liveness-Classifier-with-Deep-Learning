{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-08T02:04:08.341741Z",
     "iopub.status.busy": "2025-04-08T02:04:08.341382Z",
     "iopub.status.idle": "2025-04-08T02:08:56.322063Z",
     "shell.execute_reply": "2025-04-08T02:08:56.320928Z",
     "shell.execute_reply.started": "2025-04-08T02:04:08.341719Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Clone repository\n",
    "!git clone https://github.com/zhanghang1989/PyTorch-Encoding.git\n",
    "%cd PyTorch-Encoding\n",
    "\n",
    "# Step 2: Install dependencies (optional but good to have)\n",
    "!pip install -q yacs\n",
    "\n",
    "# Step 3: Fix deprecated .data<T>() to .data_ptr<T>() in CUDA files\n",
    "!find encoding/lib -type f -name \"*.cu\" -exec sed -i 's/\\.data<float>()/.data_ptr<float>()/g' {} \\;\n",
    "!find encoding/lib -type f -name \"*.cu\" -exec sed -i 's/\\.data<double>()/.data_ptr<double>()/g' {} \\;\n",
    "\n",
    "# Step 4: Build the extension in-place\n",
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-08T02:08:56.323946Z",
     "iopub.status.busy": "2025-04-08T02:08:56.323668Z",
     "iopub.status.idle": "2025-04-08T02:09:03.245929Z",
     "shell.execute_reply": "2025-04-08T02:09:03.244818Z",
     "shell.execute_reply.started": "2025-04-08T02:08:56.323917Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install timm\n",
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:09:03.247995Z",
     "iopub.status.busy": "2025-04-08T02:09:03.247741Z",
     "iopub.status.idle": "2025-04-08T02:09:03.251803Z",
     "shell.execute_reply": "2025-04-08T02:09:03.251025Z",
     "shell.execute_reply.started": "2025-04-08T02:09:03.247975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/PyTorch-Encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:09:03.253284Z",
     "iopub.status.busy": "2025-04-08T02:09:03.252976Z",
     "iopub.status.idle": "2025-04-08T02:09:06.583680Z",
     "shell.execute_reply": "2025-04-08T02:09:06.582767Z",
     "shell.execute_reply.started": "2025-04-08T02:09:03.253255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import all packages need\n",
    "from PIL import Image\n",
    "from sklearn.metrics import recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights, mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '6.0' #  Consider change that if you have GPU diff P100\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import encoding\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:09:06.584754Z",
     "iopub.status.busy": "2025-04-08T02:09:06.584500Z",
     "iopub.status.idle": "2025-04-08T02:09:06.588614Z",
     "shell.execute_reply": "2025-04-08T02:09:06.587651Z",
     "shell.execute_reply.started": "2025-04-08T02:09:06.584732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load & Pre-process the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:09:06.589882Z",
     "iopub.status.busy": "2025-04-08T02:09:06.589591Z",
     "iopub.status.idle": "2025-04-08T02:09:06.609775Z",
     "shell.execute_reply": "2025-04-08T02:09:06.608898Z",
     "shell.execute_reply.started": "2025-04-08T02:09:06.589855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AntiSpoofingDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        '''\n",
    "        :param root_dir: Path to the directory containing either train or dev data.\n",
    "        :param transform: Processing image.\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = [\"normal\", \"spoof\"]\n",
    "        self.class_to_idx = {\"normal\" : 0, \"spoof\" : 1}\n",
    "        self.transform = transform\n",
    "\n",
    "        self.files = [] # # Contains the image file paths and their corresponding labels.\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            for file in os.listdir(class_dir):\n",
    "                file_dir = os.path.join(class_dir, file)\n",
    "                self.files.append((file_dir, class_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_dir, label = self.files[idx]\n",
    "        img_rgb = Image.open(file_dir).convert(\"RGB\")\n",
    "        img_rgb_tensor = self.transform(img_rgb)\n",
    "        return img_rgb_tensor, label\n",
    "\n",
    "def load_data(root_dir, batch_size, transform, shuffle):\n",
    "    dataset = AntiSpoofingDataset(root_dir=root_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Custom Model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:09:06.610837Z",
     "iopub.status.busy": "2025-04-08T02:09:06.610619Z",
     "iopub.status.idle": "2025-04-08T02:09:06.627533Z",
     "shell.execute_reply": "2025-04-08T02:09:06.626683Z",
     "shell.execute_reply.started": "2025-04-08T02:09:06.610816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        '''\n",
    "        :param dropout_rate: Dropout rate\n",
    "        '''\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1) # Load pre-trained of ResNet50\n",
    "\n",
    "        self.backbone.fc = nn.Identity() # Drop the last layer\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add new FC layer: Batch norm -> Dropout -> Linear\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=2048, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomMobileNetV2(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        '''\n",
    "        :param dropout_rate: Dropout rate\n",
    "        '''\n",
    "        super(CustomMobileNetV2, self).__init__()\n",
    "        self.backbone = mobilenet_v2(MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.backbone.classifier = nn.Identity() # Drop the last layer\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add new FC layer: Batch norm -> Dropout -> Linear\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=1280),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=1280, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        '''\n",
    "        :param dropout_rate: Dropout rate\n",
    "        '''\n",
    "        super(CustomViT, self).__init__()\n",
    "        self.backbone = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n",
    "\n",
    "        self.backbone.head = nn.Identity() # Drop the last layer\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Add new FC layer: Batch norm -> Dropout -> Linear\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=768),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=768, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomTextureModel(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        '''\n",
    "        :param dropout_rate: Dropout rate\n",
    "        '''\n",
    "        super(CustomTextureModel, self).__init__()\n",
    "\n",
    "        # Load the pre-trained model (DeepTen with ResNet backbone)\n",
    "        self.backbone = encoding.models.get_model(\"deepten_resnet50_minc\", pretrained=True)\n",
    "\n",
    "        self.backbone.head[5] = nn.Identity()\n",
    "        self.backbone.head[6] = nn.Identity()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features=4096),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=4096, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Freeze the weights of the pretrained backbone layers\n",
    "        for param in self.backbone.pretrained.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:17:35.750537Z",
     "iopub.status.busy": "2025-04-08T02:17:35.750225Z",
     "iopub.status.idle": "2025-04-08T02:17:35.754551Z",
     "shell.execute_reply": "2025-04-08T02:17:35.753562Z",
     "shell.execute_reply.started": "2025-04-08T02:17:35.750513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fixed randomization to ensure fairness in comparing methods\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T02:17:37.115254Z",
     "iopub.status.busy": "2025-04-08T02:17:37.114922Z",
     "iopub.status.idle": "2025-04-08T02:17:37.140695Z",
     "shell.execute_reply": "2025-04-08T02:17:37.139652Z",
     "shell.execute_reply.started": "2025-04-08T02:17:37.115232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validate model and calculate loss for per batch\n",
    "def validate_model(dev_dataloader, classify_model, texture_model, criterion, device, alpha):\n",
    "    classify_model.eval()\n",
    "    texture_model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in dev_dataloader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out1, out2 = classify_model(img), texture_model(img)\n",
    "            out = alpha * out1 + (1 - alpha) * out2\n",
    "            loss = criterion(out, label.unsqueeze(1).float())\n",
    "            total_loss += loss.item()\n",
    "    loss_val = total_loss / len(dev_dataloader)\n",
    "    return loss_val\n",
    "\n",
    "# Training model per epoch and calculate loss for per batch\n",
    "def train_per_epoch(classify_model, texture_model, train_dataloader, criterion, classify_optimizer, texture_optimizer, device, epoch, num_epochs, alpha):\n",
    "    classify_model.train()\n",
    "    texture_model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for img, label in progress_bar:\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        classify_optimizer.zero_grad()\n",
    "        texture_optimizer.zero_grad()\n",
    "        \n",
    "        out1, out2 = classify_model(img), texture_model(img)\n",
    "        out = alpha * out1 + (1 - alpha) * out2\n",
    "        loss = criterion(out, label.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        \n",
    "        classify_optimizer.step()\n",
    "        texture_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "# Save loss of training and validating\n",
    "def save_plots(train_losses, val_losses, model_name, folder_ckpt, alpha):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "    plt.title(f\"Train and Validation Loss - {model_name} with alpha = {alpha}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(folder_ckpt, f\"{alpha}_{model_name}_train_val_loss.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# Load model and setting layer to fine-tuning\n",
    "def get_model(model_name):\n",
    "    models = {\n",
    "        \"ResNet50\": CustomResNet50,\n",
    "        \"MobileNetV2\": CustomMobileNetV2,\n",
    "        \"ViT\": CustomViT\n",
    "    }\n",
    "\n",
    "    if model_name not in models:\n",
    "        raise ValueError(f\"Model should be one of {list(models.keys())}, get {model_name}.\")\n",
    "    return models[model_name]()\n",
    "\n",
    "# Show training information like: num_epochs, batch_size, model, ...\n",
    "def print_training_infor(model_name, num_epochs, lr, batch_size, alpha):\n",
    "    print(60 * \"-\")\n",
    "    if alpha == 1:\n",
    "        model_info = \"Don't use Texture Model\"\n",
    "    elif alpha == 0:\n",
    "        model_info = \"Don't use Classification Model\"\n",
    "    else:\n",
    "        model_info = \"Ensemble Classification and Texture Models\"\n",
    "    \n",
    "    print(f\"Training `{model_name}` with {model_info} (alpha = {alpha})\")\n",
    "    print(f\"Epochs: {num_epochs} | Batch size: {batch_size} | Init Learning rate: {lr}\")\n",
    "    print(60 * \"-\")\n",
    "\n",
    "# Train model\n",
    "def train_model(train_root_dir, dev_root_dir, model_name, num_epochs, lr, batch_size, folder_ckpt, alpha, beta, theta):\n",
    "    # Make a folder save checkpoint, image loss, ...\n",
    "    os.makedirs(folder_ckpt, exist_ok=True)\n",
    "\n",
    "    # Load classify model and texture model\n",
    "    classify_model = get_model(model_name)\n",
    "    texture_model = CustomTextureModel()\n",
    "\n",
    "    # Pre-prare data: Data Augmentation for Training Set, Transform for Validation/Test Set\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.15,\n",
    "            contrast=0.15\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dev_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataloader = load_data(root_dir=train_root_dir, batch_size=batch_size, transform=train_transform, shuffle=True)\n",
    "    dev_dataloader = load_data(root_dir=dev_root_dir, batch_size=batch_size, transform=dev_transform, shuffle=False)\n",
    "\n",
    "    # Show information training\n",
    "    print_training_infor(model_name, num_epochs, lr, batch_size, alpha)\n",
    "\n",
    "    # Setting device, loss function and optimizer\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    classify_model = classify_model.to(device)\n",
    "    texture_model = texture_model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    classify_optimizer = optim.Adam(classify_model.parameters(), lr)\n",
    "    texture_optimizer = optim.Adam(texture_model.parameters(), lr)\n",
    "\n",
    "    # Add ReduceLROnPlateau scheduler\n",
    "    classify_scheduler = optim.lr_scheduler.ReduceLROnPlateau(classify_optimizer, mode='min', factor=0.9, patience=3, verbose=True)\n",
    "    texture_scheduler = optim.lr_scheduler.ReduceLROnPlateau(texture_optimizer, mode='min', factor=0.9, patience=3, verbose=True)\n",
    "\n",
    "    # Flow train and validate phase\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training 1 epoch\n",
    "        train_loss = train_per_epoch(\n",
    "            classify_model, texture_model, train_dataloader, criterion, classify_optimizer, texture_optimizer, device, epoch, num_epochs, alpha\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate in validation set\n",
    "        val_loss = validate_model(dev_dataloader, classify_model, texture_model, criterion, device, alpha)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Step scheduler based on val_loss\n",
    "        classify_scheduler.step(val_loss)\n",
    "        texture_scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        # Show information per epoch\n",
    "        print(f\"Train loss = {train_loss:.4f} | Val loss = {val_loss:.4f} | Learning rate = {classify_optimizer.param_groups[0]['lr']:.7f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            # Save the best model\n",
    "            best_classify_model_dir = os.path.join(folder_ckpt, f\"{alpha}_{model_name}_best.pt\")\n",
    "            torch.save(classify_model.state_dict(), best_classify_model_dir)\n",
    "\n",
    "            best_texture_model_dir = os.path.join(folder_ckpt, f\"{alpha}_texture_best.pt\")\n",
    "            torch.save(texture_model.state_dict(), best_texture_model_dir)\n",
    "\n",
    "            print(f\"Best model saved at Epoch {epoch+1}.\\n\")\n",
    "\n",
    "        # Save the last model\n",
    "        last_classify_model_dir = os.path.join(folder_ckpt, f\"{alpha}_{model_name}_last.pt\")\n",
    "        torch.save(classify_model.state_dict(), last_classify_model_dir)\n",
    "\n",
    "        last_texture_model_dir = os.path.join(folder_ckpt, f\"{alpha}_texture_last.pt\")\n",
    "        torch.save(texture_model.state_dict(), last_texture_model_dir)\n",
    "\n",
    "    # Save plot of train/validate loss\n",
    "    save_plots(train_losses, val_losses, model_name, folder_ckpt, alpha)\n",
    "\n",
    "    # We find the best threshold for metric\n",
    "    find_best_threshold(model_name, dev_dataloader, device, folder_ckpt, alpha, beta, theta)\n",
    "\n",
    "def plot_metric_follow_threshold(thresholds, metrics, color, title, xlabel, y_label, filename, folder_ckpt, theta=None):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(thresholds, metrics, color=color, linewidth=2)\n",
    "    if theta:\n",
    "        plt.axhline(y=theta, color=\"cyan\", linestyle=\"--\", linewidth=2, label=f\"{y_label} threshold\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(folder_ckpt, filename))\n",
    "    plt.close()\n",
    "\n",
    "# Find the threshold to get best metrics\n",
    "def find_best_threshold(model_name, dev_dataloader, device, folder_ckpt, alpha, beta, theta):\n",
    "    classify_model = get_model(model_name)\n",
    "    best_classify_model_dir = os.path.join(folder_ckpt, f\"{alpha}_{model_name}_best.pt\")\n",
    "    classify_model.load_state_dict(torch.load(best_classify_model_dir, map_location=device, weights_only=True))\n",
    "\n",
    "    texture_model = CustomTextureModel()\n",
    "    best_texture_model_dir = os.path.join(folder_ckpt, f\"{alpha}_texture_best.pt\")\n",
    "    texture_model.load_state_dict(torch.load(best_texture_model_dir, map_location=device, weights_only=True))\n",
    "\n",
    "    classify_model.to(device)\n",
    "    classify_model.eval()\n",
    "\n",
    "    texture_model.to(device)\n",
    "    texture_model.eval()\n",
    "\n",
    "    thresholds = []\n",
    "    recalls = []\n",
    "    accus = []\n",
    "    bpcers = []\n",
    "\n",
    "    y_true = []\n",
    "    y_probability = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in dev_dataloader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out1, out2 = classify_model(img).squeeze(1), texture_model(img).squeeze(1)\n",
    "            out = alpha * out1 + (1 - alpha) * out2\n",
    "            y_probability.extend(out.cpu().numpy())\n",
    "            y_true.extend(label.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_probability = np.array(y_probability)\n",
    "\n",
    "    best_recall = 0 # maximum recall but keep bpcer <= beta, accuracy >= theta\n",
    "    best_bpcer = 1\n",
    "    best_accu = 0\n",
    "    best_threshold = 0\n",
    "\n",
    "    total_real_images = np.sum(y_true == 0)\n",
    "    for i in range(0, 100, 1):\n",
    "        threshold = i / 100\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "        y_pred = (y_probability >= threshold).astype(int)\n",
    "\n",
    "        # Recall: ability to correctly detect spoof (label=1)\n",
    "        recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        # Acuracy\n",
    "        accu = accuracy_score(y_true, y_pred)\n",
    "        accus.append(accu)\n",
    "\n",
    "        # BPCER: % real images (label=0) wrongly predicted as fake\n",
    "        FP = sum(1 for y, y_hat in zip(y_true, y_pred) if y_hat == 1 and y == 0) # Total real image but model predict fake\n",
    "\n",
    "        bpcer = 0\n",
    "        if total_real_images != 0:\n",
    "            bpcer = FP / total_real_images\n",
    "        bpcers.append(bpcer)\n",
    "\n",
    "        if  bpcer <= beta and accu >= theta:\n",
    "            if recall > best_recall:\n",
    "                best_recall, best_bpcer, best_accu, best_threshold = recall, bpcer, accu, threshold\n",
    "\n",
    "    if best_recall == 0:\n",
    "        print(\"Don't have any value of threshold make best metric.\")\n",
    "    else:\n",
    "        print(f\"At threshold = {best_threshold}: Recall = {best_recall:.4f}, BPCER = {best_bpcer:.4f}, Accuracy = {best_accu:.4f}\")\n",
    "\n",
    "    # Plot of metrics for every threshold values\n",
    "    plot_metric_follow_threshold(thresholds, recalls, \"blue\", \"Recall vs Threshold\", \"Threshold\", \"Recall\", f\"{alpha}_{model_name}_recall_threshold.png\", folder_ckpt)\n",
    "    plot_metric_follow_threshold(thresholds, bpcers, \"red\", \"BPCER vs Threshold\", \"Threshold\", \"BPCER\", f\"{alpha}_{model_name}_bpcer_threshold.png\", folder_ckpt, beta)\n",
    "    plot_metric_follow_threshold(thresholds, accus, \"yellow\", \"Accuracy vs Threshold\", \"Threshold\", \"Accuracy\", f\"{alpha}_{model_name}_accuracy_threshold.png\", folder_ckpt, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:50:28.290801Z",
     "iopub.status.busy": "2025-04-08T07:50:28.290450Z",
     "iopub.status.idle": "2025-04-08T09:05:42.047402Z",
     "shell.execute_reply": "2025-04-08T09:05:42.046529Z",
     "shell.execute_reply.started": "2025-04-08T07:50:28.290773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Training `ViT` with Ensemble Classification and Texture Models (alpha = 0.5)\n",
      "Epochs: 40 | Batch size: 128 | Init Learning rate: 0.001\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40: 100%|██████████| 25/25 [01:27<00:00,  3.50s/batch, loss=0.2484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.4188 | Val loss = 0.4371 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 25/25 [01:23<00:00,  3.35s/batch, loss=0.2225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.2729 | Val loss = 0.3577 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 2.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 25/25 [01:23<00:00,  3.33s/batch, loss=0.2155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.2246 | Val loss = 0.3290 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 3.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 25/25 [01:22<00:00,  3.31s/batch, loss=0.1619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1888 | Val loss = 0.3089 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 4.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 25/25 [01:22<00:00,  3.32s/batch, loss=0.1640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1615 | Val loss = 0.3031 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 5.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 25/25 [01:24<00:00,  3.36s/batch, loss=0.1222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1488 | Val loss = 0.3018 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 6.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 25/25 [01:23<00:00,  3.34s/batch, loss=0.1056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1449 | Val loss = 0.3004 | Learning rate = 0.0010000\n",
      "Best model saved at Epoch 7.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 25/25 [01:23<00:00,  3.34s/batch, loss=0.1955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1349 | Val loss = 0.3007 | Learning rate = 0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 25/25 [01:23<00:00,  3.33s/batch, loss=0.1667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1341 | Val loss = 0.3148 | Learning rate = 0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 25/25 [01:21<00:00,  3.24s/batch, loss=0.1420]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1325 | Val loss = 0.3044 | Learning rate = 0.0010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 25/25 [01:21<00:00,  3.26s/batch, loss=0.0885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1274 | Val loss = 0.3226 | Learning rate = 0.0009000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40: 100%|██████████| 25/25 [01:22<00:00,  3.28s/batch, loss=0.1336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1225 | Val loss = 0.2794 | Learning rate = 0.0009000\n",
      "Best model saved at Epoch 12.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.1197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1183 | Val loss = 0.3073 | Learning rate = 0.0009000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.1177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1183 | Val loss = 0.3129 | Learning rate = 0.0009000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.1043]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1107 | Val loss = 0.2847 | Learning rate = 0.0009000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40: 100%|██████████| 25/25 [01:21<00:00,  3.26s/batch, loss=0.1098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1142 | Val loss = 0.2973 | Learning rate = 0.0008100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40: 100%|██████████| 25/25 [01:21<00:00,  3.24s/batch, loss=0.0693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1130 | Val loss = 0.2954 | Learning rate = 0.0008100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40: 100%|██████████| 25/25 [01:21<00:00,  3.24s/batch, loss=0.1078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1131 | Val loss = 0.3023 | Learning rate = 0.0008100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.1308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1085 | Val loss = 0.2942 | Learning rate = 0.0008100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.0824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1113 | Val loss = 0.2973 | Learning rate = 0.0007290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.0808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0989 | Val loss = 0.3004 | Learning rate = 0.0007290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.0658]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1047 | Val loss = 0.3001 | Learning rate = 0.0007290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40: 100%|██████████| 25/25 [01:20<00:00,  3.23s/batch, loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1010 | Val loss = 0.2934 | Learning rate = 0.0007290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40: 100%|██████████| 25/25 [01:20<00:00,  3.23s/batch, loss=0.1081]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1048 | Val loss = 0.3121 | Learning rate = 0.0006561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.0991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0999 | Val loss = 0.2849 | Learning rate = 0.0006561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40: 100%|██████████| 25/25 [01:21<00:00,  3.24s/batch, loss=0.0637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0976 | Val loss = 0.2987 | Learning rate = 0.0006561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40: 100%|██████████| 25/25 [01:20<00:00,  3.23s/batch, loss=0.1331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1023 | Val loss = 0.3011 | Learning rate = 0.0006561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40: 100%|██████████| 25/25 [01:22<00:00,  3.28s/batch, loss=0.1287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0977 | Val loss = 0.3145 | Learning rate = 0.0005905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40: 100%|██████████| 25/25 [01:22<00:00,  3.28s/batch, loss=0.1160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.1017 | Val loss = 0.2923 | Learning rate = 0.0005905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40: 100%|██████████| 25/25 [01:22<00:00,  3.29s/batch, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0968 | Val loss = 0.3007 | Learning rate = 0.0005905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40: 100%|██████████| 25/25 [01:22<00:00,  3.32s/batch, loss=0.1038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0934 | Val loss = 0.3039 | Learning rate = 0.0005905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.1232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0958 | Val loss = 0.3103 | Learning rate = 0.0005314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40: 100%|██████████| 25/25 [01:22<00:00,  3.30s/batch, loss=0.1045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0905 | Val loss = 0.3142 | Learning rate = 0.0005314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40: 100%|██████████| 25/25 [01:22<00:00,  3.32s/batch, loss=0.1157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0949 | Val loss = 0.3108 | Learning rate = 0.0005314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40: 100%|██████████| 25/25 [01:22<00:00,  3.31s/batch, loss=0.1123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0999 | Val loss = 0.3087 | Learning rate = 0.0005314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40: 100%|██████████| 25/25 [01:22<00:00,  3.31s/batch, loss=0.1053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0983 | Val loss = 0.3126 | Learning rate = 0.0004783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40: 100%|██████████| 25/25 [01:22<00:00,  3.28s/batch, loss=0.1148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0933 | Val loss = 0.3120 | Learning rate = 0.0004783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.0976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0966 | Val loss = 0.3001 | Learning rate = 0.0004783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40: 100%|██████████| 25/25 [01:21<00:00,  3.27s/batch, loss=0.0590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0945 | Val loss = 0.3047 | Learning rate = 0.0004783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40: 100%|██████████| 25/25 [01:21<00:00,  3.25s/batch, loss=0.0907]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss = 0.0924 | Val loss = 0.2964 | Learning rate = 0.0004305\n",
      "At threshold = 0.27: Recall = 0.9585, BPCER = 0.1944, Accuracy = 0.8821\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    train_root_dir=\"/kaggle/input/fas-dataset/train\", \n",
    "    dev_root_dir=\"/kaggle/input/fas-dataset/dev\", \n",
    "    model_name=\"ViT\", \n",
    "    num_epochs=40, \n",
    "    lr=0.001, \n",
    "    batch_size=128, \n",
    "    folder_ckpt=\"/kaggle/working/\", \n",
    "    alpha=0.5, \n",
    "    beta=0.20, \n",
    "    theta=0.85\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate time end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T05:53:12.208989Z",
     "iopub.status.busy": "2025-04-08T05:53:12.208663Z",
     "iopub.status.idle": "2025-04-08T05:53:12.212676Z",
     "shell.execute_reply": "2025-04-08T05:53:12.211835Z",
     "shell.execute_reply.started": "2025-04-08T05:53:12.208965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T05:53:13.659781Z",
     "iopub.status.busy": "2025-04-08T05:53:13.659467Z",
     "iopub.status.idle": "2025-04-08T05:53:13.666985Z",
     "shell.execute_reply": "2025-04-08T05:53:13.666021Z",
     "shell.execute_reply.started": "2025-04-08T05:53:13.659757Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def measure_inference_time_dataloader(model_name, root_dir, alpha, device='cuda'):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    classify_model = get_model(model_name).to(device)\n",
    "    classify_model.eval()\n",
    "\n",
    "    texture_model = CustomTextureModel().to(device)\n",
    "    texture_model.eval()\n",
    "\n",
    "    total_time = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    dev_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataloader = load_data(root_dir, batch_size=128, transform=dev_transform, shuffle=False)\n",
    "    \n",
    "    # Calculate time for all dataset\n",
    "    with torch.no_grad():\n",
    "        for img, _ in dataloader:\n",
    "            img = img.to(device)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            start_time = time.time()\n",
    "\n",
    "            out1, out2 = classify_model(img), texture_model(img)\n",
    "            out = alpha * out1 + (1 - alpha) * out2\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "\n",
    "            batch_time = end_time - start_time\n",
    "            total_time += batch_time\n",
    "            total_samples += img.size(0)\n",
    "\n",
    "    avg_time_per_sample = total_time / total_samples\n",
    "    print(f\"[{device}] Average inference time per sample: {avg_time_per_sample:.6f} seconds\")\n",
    "    return avg_time_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T09:06:06.185537Z",
     "iopub.status.busy": "2025-04-08T09:06:06.185198Z",
     "iopub.status.idle": "2025-04-08T09:06:36.183268Z",
     "shell.execute_reply": "2025-04-08T09:06:36.182594Z",
     "shell.execute_reply.started": "2025-04-08T09:06:06.185512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda] Average inference time per sample: 0.007237 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.007237450625017236"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_inference_time_dataloader(model_name=\"ViT\", alpha=0.5, root_dir=\"/kaggle/input/fas-dataset/dev\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T07:50:10.854757Z",
     "iopub.status.busy": "2025-04-08T07:50:10.854380Z",
     "iopub.status.idle": "2025-04-08T07:50:12.868011Z",
     "shell.execute_reply": "2025-04-08T07:50:12.866696Z",
     "shell.execute_reply.started": "2025-04-08T07:50:10.854725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/kaggle/working/1_ViT_accuracy_threshold.png': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_best.pt': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_bpcer_threshold.png': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_last.pt': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_recall_threshold.png': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_train_val_loss.png': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_best.pt': No such file or directory\n",
      "rm: cannot remove '/kaggle/working/1_ViT_last.pt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm \"/kaggle/working/1_ViT_accuracy_threshold.png\"\n",
    "!rm \"/kaggle/working/1_ViT_best.pt\"\n",
    "!rm \"/kaggle/working/1_ViT_bpcer_threshold.png\"\n",
    "!rm \"/kaggle/working/1_ViT_last.pt\"\n",
    "!rm \"/kaggle/working/1_ViT_recall_threshold.png\"\n",
    "!rm \"/kaggle/working/1_ViT_train_val_loss.png\"\n",
    "!rm \"/kaggle/working/1_ViT_best.pt\"\n",
    "!rm \"/kaggle/working/1_ViT_last.pt\"\n",
    "!rm \"/kaggle/working/1_texture_best.pt\"\n",
    "!rm \"/kaggle/working/1_texture_last.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7044936,
     "sourceId": 11270140,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
